{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydataset import data\n",
    "from math import sqrt\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, expr \n",
    "from pyspark.sql.functions import round, concat, sum, min, max, count, avg, mean, lit\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "\n",
    "# It is very common to see something like:\n",
    "# from pyspark.sql.functions import *\n",
    "# which will import all of the functions from the pyspark.sql.functions module.\n",
    "\n",
    "# It is also very common to see something like:\n",
    "# import pyspark.sql.functions as F\n",
    "# which will import all of the functions from the pyspark.sql.functions module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes\n",
    "- Look like pandas dataframes\n",
    "- Share some of the same methods and syntax\n",
    "- But they are 2 seperate types of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1c1</td>\n",
       "      <td>r1c2</td>\n",
       "      <td>r1c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2c1</td>\n",
       "      <td>r2c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r3c1</td>\n",
       "      <td>r3c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2  col3\n",
       "1  r1c1  r1c2  r1c3\n",
       "2  r2c1  r2c2  r3c3\n",
       "3  r3c1  r3c2  r3c3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pandas dataframe by columns using dictionary-like object\n",
    "\n",
    "pd_df = pd.DataFrame({'col1': ['r1c1', 'r2c1', 'r3c1'],\n",
    "                      'col2': ['r1c2', 'r2c2', 'r3c2'],\n",
    "                      'col3': ['r1c3', 'r3c3', 'r3c3']\n",
    "                        }, \n",
    "                     index = [1, 2, 3])\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r1c1</td>\n",
       "      <td>r1c2</td>\n",
       "      <td>r1c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2c1</td>\n",
       "      <td>r2c2</td>\n",
       "      <td>r2c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r3c1</td>\n",
       "      <td>r3c2</td>\n",
       "      <td>r3c3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2  col3\n",
       "1  r1c1  r1c2  r1c3\n",
       "2  r2c1  r2c2  r2c3\n",
       "3  r3c1  r3c2  r3c3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas dataframe by rows\n",
    "\n",
    "pd_df = pd.DataFrame([['r1c1', 'r1c2', 'r1c3'], \n",
    "                      ['r2c1', 'r2c2', 'r2c3'], \n",
    "                      ['r3c1', 'r3c2', 'r3c3']\n",
    "                      ], \n",
    "                     index = [1, 2, 3], \n",
    "                     columns = ['col1', 'col2', 'col3'])\n",
    "\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: string, col2: string, col3: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark dataframe from Pandas dataframe using spark.createDataFrame()\n",
    "\n",
    "sp_df = spark.createDataFrame(pd_df)\n",
    "sp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**<br>\n",
    "Notice that, while we do see the column names, we don't see the data in the dataframe like we would with a pandas dataframe. This is because **spark is lazy**, in that it won't show us values until it has to. For the purposes of looking at the first few rows of our data, we can use the `.show` method (defaults of 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data in the pyspark dataframe by `.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|r1c1|r1c2|r1c3|\n",
      "|r2c1|r2c2|r2c3|\n",
      "|r3c1|r3c2|r3c3|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, col1: string, col2: string, col3: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like pandas dataframes, spark dataframes have a .describe() method:\n",
    "\n",
    "sp_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**<br>\n",
    "Also like pandas, returns another dataframe. However, since this is a spark dataframe, we have to explicitly show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+\n",
      "|summary|col1|col2|col3|\n",
      "+-------+----+----+----+\n",
      "|  count|   3|   3|   3|\n",
      "|   mean|null|null|null|\n",
      "| stddev|null|null|null|\n",
      "|    min|r1c1|r1c2|r1c3|\n",
      "|    max|r3c1|r3c2|r3c3|\n",
      "+-------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default spark will show the first 20 rows, but we can specify how many we want by passing a number to `.show`. Let's use some different data so that we have a more robust dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the mpg data in spark dataframe format\n",
    "\n",
    "mpg = spark.createDataFrame(data('mpg'))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(manufacturer='audi', model='a4', displ=1.8, year=1999, cyl=4, trans='auto(l5)', drv='f', cty=18, hwy=29, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=1.8, year=1999, cyl=4, trans='manual(m5)', drv='f', cty=21, hwy=29, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.0, year=2008, cyl=4, trans='manual(m6)', drv='f', cty=20, hwy=31, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.0, year=2008, cyl=4, trans='auto(av)', drv='f', cty=21, hwy=30, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.8, year=1999, cyl=6, trans='auto(l5)', drv='f', cty=16, hwy=26, fl='p', class='compact')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp_df.head() method\n",
    "\n",
    "mpg.head(5) # Return a list of pyspark.sql.types.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(manufacturer='audi', model='a4', displ=1.8, year=1999, cyl=4, trans='auto(l5)', drv='f', cty=18, hwy=29, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=1.8, year=1999, cyl=4, trans='manual(m5)', drv='f', cty=21, hwy=29, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.0, year=2008, cyl=4, trans='manual(m6)', drv='f', cty=20, hwy=31, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.0, year=2008, cyl=4, trans='auto(av)', drv='f', cty=21, hwy=30, fl='p', class='compact'),\n",
       " Row(manufacturer='audi', model='a4', displ=2.8, year=1999, cyl=6, trans='auto(l5)', drv='f', cty=16, hwy=26, fl='p', class='compact')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp_df.take() method\n",
    "\n",
    "mpg.take(5) # Return a list of pyspark.sql.types.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|summary|manufacturer|            model|             displ|             year|              cyl|     trans|drv|               cty|              hwy|  fl|  class|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|  count|         234|              234|               234|              234|              234|       234|234|               234|              234| 234|    234|\n",
      "|   mean|        null|             null| 3.471794871794872|           2003.5|5.888888888888889|      null|4.0|16.858974358974358|23.44017094017094|null|   null|\n",
      "| stddev|        null|             null|1.2919590310839348|4.509646313320436|1.611534484684289|      null|0.0| 4.255945678889394|5.954643441166448|null|   null|\n",
      "|    min|        audi|      4runner 4wd|               1.6|             1999|                4|  auto(av)|  4|                 9|               12|   c|2seater|\n",
      "|    max|  volkswagen|toyota tacoma 4wd|               7.0|             2008|                8|manual(m6)|  r|                35|               44|   r|    suv|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sp_df.describe()\n",
    "\n",
    "mpg.describe().show() # Not quite readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manufacturer',\n",
       " 'model',\n",
       " 'displ',\n",
       " 'year',\n",
       " 'cyl',\n",
       " 'trans',\n",
       " 'drv',\n",
       " 'cty',\n",
       " 'hwy',\n",
       " 'fl',\n",
       " 'class']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp_df.columns\n",
    "\n",
    "mpg.columns # returns the list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp_df.count()\n",
    "\n",
    "mpg.count() # returns the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sp_df.distinct().count()\n",
    "\n",
    "mpg.distinct().count() # returns the number of distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sp_df.printSchema()\n",
    "\n",
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 234 entries, 1 to 234\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   manufacturer  234 non-null    object \n",
      " 1   model         234 non-null    object \n",
      " 2   displ         234 non-null    float64\n",
      " 3   year          234 non-null    int64  \n",
      " 4   cyl           234 non-null    int64  \n",
      " 5   trans         234 non-null    object \n",
      " 6   drv           234 non-null    object \n",
      " 7   cty           234 non-null    int64  \n",
      " 8   hwy           234 non-null    int64  \n",
      " 9   fl            234 non-null    object \n",
      " 10  class         234 non-null    object \n",
      "dtypes: float64(1), int64(4), object(6)\n",
      "memory usage: 21.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the mpg in pandas df\n",
    "\n",
    "df = data('mpg')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "- Pandas series vs. Spark column objects\n",
    "- A column object represents a vertical slice of a dataframe, but does not contain the data itself.\n",
    "- You will use it to perform functions on and reference that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pyspark dataframe column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    a4\n",
       "2    a4\n",
       "3    a4\n",
       "4    a4\n",
       "5    a4\n",
       "Name: model, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas series\n",
    "\n",
    "df.model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'model'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark df column object\n",
    "\n",
    "mpg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative way to create a spark df column object\n",
    "\n",
    "mpg['hwy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4bdf227cc276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sp_df column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 'Column' object is not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "# sp_df column\n",
    "\n",
    "mpg.model.show() # 'Column' object is not callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While this expression would produce a Series of values from a pandas dataframe, for a spark dataframe this produces a Column object, which is an object that represents a vertical slice of a dataframe, but does not contain the data itself.\n",
    "\n",
    "- One way to use our column objects is to use them in combination with the `.select` method. `.select` is very powerful, and lets us specify what data we want to see in the resulting dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To speficy the data we want by using `select()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hwy</th>\n",
       "      <th>cty</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>a4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hwy  cty model\n",
       "1   29   18    a4\n",
       "2   29   21    a4\n",
       "3   31   20    a4\n",
       "4   30   21    a4\n",
       "5   26   16    a4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns hwy, cty, and model in pandas df\n",
    "\n",
    "df[['hwy', 'cty', 'model']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, model: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns hwy, cty, and model (Create a spark dataframe object)\n",
    "\n",
    "mpg.select(mpg.hwy, mpg.cty, mpg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice that we don't see any data, instead we see the new dataframe that is produced. To see the actual data, we'll again need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|hwy|cty|model|\n",
      "+---+---+-----+\n",
      "| 29| 18|   a4|\n",
      "| 29| 21|   a4|\n",
      "| 31| 20|   a4|\n",
      "| 30| 21|   a4|\n",
      "| 26| 16|   a4|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select and show columns\n",
    "\n",
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|hwy|cty|model|\n",
      "+---+---+-----+\n",
      "| 29| 18|   a4|\n",
      "| 29| 21|   a4|\n",
      "| 31| 20|   a4|\n",
      "| 30| 21|   a4|\n",
      "| 26| 16|   a4|\n",
      "+---+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative way to select and show spark df columns\n",
    "\n",
    "mpg.select('hwy', 'cty', 'model').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column objects support operations such as arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    30\n",
       "2    30\n",
       "3    32\n",
       "4    31\n",
       "5    27\n",
       "Name: hwy, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas Series support arithmetic operations\n",
    "\n",
    "(df.hwy + 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(hwy + 1)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark df columns object also support arithmetic operations\n",
    "\n",
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get back a column that represents the values from the original hwy column with 1 added to them. To actually see this data, we'd need to select it and show the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "|hwy|(hwy + 1)|(hwy * 5)|\n",
      "+---+---------+---------+\n",
      "| 29|       30|      145|\n",
      "| 29|       30|      145|\n",
      "| 31|       32|      155|\n",
      "| 30|       31|      150|\n",
      "| 26|       27|      130|\n",
      "+---+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy + 1, mpg.hwy*5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT work\n",
    "mpg.select('hwy', 'hwy' + 1, 'hwy'*5).show(5) \n",
    "\n",
    "# TypeError: can only concatenate str (not \"int\") to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "|hwy|(hwy + 1)|(hwy * 5)|\n",
      "+---+---------+---------+\n",
      "| 29|       30|      145|\n",
      "| 29|       30|      145|\n",
      "| 31|       32|      155|\n",
      "| 30|       31|      150|\n",
      "| 26|       27|      130|\n",
      "+---+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# An alternative way\n",
    "\n",
    "mpg.select(mpg['hwy'], mpg['hwy'] + 1, mpg['hwy']*5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if spark column object + \"string\"? No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|model|(model + string)|\n",
      "+-----+----------------+\n",
      "|   a4|            null|\n",
      "|   a4|            null|\n",
      "|   a4|            null|\n",
      "|   a4|            null|\n",
      "|   a4|            null|\n",
      "+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.model, mpg.model + \"string\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the column object using the `.alias` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|highway_mileage|highway_mileage_plus1|\n",
      "+---------------+---------------------+\n",
      "|             29|                   30|\n",
      "|             29|                   30|\n",
      "|             31|                   32|\n",
      "|             30|                   31|\n",
      "|             26|                   27|\n",
      "+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.hwy.alias('highway_mileage'),\n",
    "           (mpg.hwy+1).alias('highway_mileage_plus1')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store column objects and reference them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "|             30|                  15.0|\n",
      "|             26|                  13.0|\n",
      "+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col1 = mpg.hwy.alias('highway_mileage')\n",
    "col2 = (mpg.hwy/2).alias('highway_mileage_halved')\n",
    "\n",
    "mpg.select(col1, col2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `col` functions to create columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the hwy column object\n",
    "\n",
    "col('hwy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the hwy column object\n",
    "\n",
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'hwy'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the hwy column object\n",
    "\n",
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The column object produced by the col function is the same as the the previous column object we saw.\n",
    "- We can create avg_mileage using the col function to produce pyspark Column objects and using the arithmetic operators to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((hwy + cty) / 2)'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the pyspark column object avg_col \n",
    "\n",
    "avg_col = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "avg_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|hwy_mileage|cty_mileage|avg_mileage|\n",
      "+-----------+-----------+-----------+\n",
      "|         29|         18|       23.5|\n",
      "|         29|         21|       25.0|\n",
      "|         31|         20|       25.5|\n",
      "|         30|         21|       25.5|\n",
      "|         26|         16|       21.0|\n",
      "+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of the selected columns\n",
    "\n",
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"hwy_mileage\"), \n",
    "    mpg.cty.alias(\"cty_mileage\"), \n",
    "    avg_col.alias('avg_mileage')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `expr` functions to create columns\n",
    "- The `expr` function is more powerful than col. \n",
    "- It does everything col does and more. \n",
    "- `expr` returns the same type of column object, but allows us to express manipulations to the column within the string that defines the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+---------------+\n",
      "|hwy|(hwy + 1)|highway_mileage|highway_mileage|\n",
      "+---+---------+---------------+---------------+\n",
      "| 29|       30|             29|             30|\n",
      "| 29|       30|             29|             30|\n",
      "| 31|       32|             31|             32|\n",
      "| 30|       31|             30|             31|\n",
      "| 26|       27|             26|             27|\n",
      "+---+---------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    expr(\"hwy\"), \n",
    "    expr(\"hwy + 1\"),\n",
    "    expr(\"hwy AS highway_mileage\"), \n",
    "    expr(\"hwy + 1 AS highway_mileage\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the columns created below are identical, and which syntax to use is merely a style choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "|     30|     30|     30|     30|\n",
      "|     26|     26|     26|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "- As we've seen through the column definitions, spark is very flexible and allows us many different ways to express ourselves. \n",
    "- Another way that is fairly different than what we've seen above is through spark SQL, which lets us write SQL queries against our spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to start using spark SQL, we'll first \"register\" the table with spark\n",
    "\n",
    "mpg.createOrReplaceTempView(\"mpg_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What if I try to print the mpg_view? \n",
    "\n",
    "# mpg_view\n",
    "\n",
    "# # NameError: name 'mpg_view' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a sql querry against the mpg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, avg: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty)/2 AS avg\n",
    "    from mpg_view\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting value is another dataframe. As we know, in order to view the values in a dataframe, we need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "| 31| 20|25.5|\n",
      "| 30| 21|25.5|\n",
      "| 26| 16|21.0|\n",
      "+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT hwy, cty, (hwy + cty)/2 AS avg\n",
    "    from mpg_view\n",
    "    \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** All of these methods for creating / manipulating dataframes are the same in terms of performance. The resulting dataframes get turned into the same spark code that gets executed on the JVM, so it really is just a style choice as to which to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting\n",
    "**View column datatypes** using `dtypes` or `printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **convert** from one type to another, we can use the `.cast` method on a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hwy', 'bigint'), ('hwy', 'string')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy,\n",
    "    mpg.hwy.cast(\"string\")\n",
    ").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy,\n",
    "    mpg.hwy.cast(\"string\")\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a value is not able to be converted, it will be replaced with null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|model|\n",
      "+-----+-----+\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.model, \n",
    "    col(\"model\").cast(\"int\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Built-in Functions\n",
    "There are many other functions beyong `col` and `expr` within the `pyspark.sql.functions` module for operating on pyspark dataframe columns.\n",
    "- `concat`: to concatenate strings\n",
    "- `sum`: to sum a group\n",
    "- `avg`: to take the avereage of a group\n",
    "- `min`: to find the minimum\n",
    "- `max`: to find the maximum\n",
    "\n",
    "**Note that importing the `sum` function directly will override the built-in sum function.** This means you will get an error if you try to sum a list of numbers, because sum will refernce the pyspark sum function, which works with pyspark dataframe columns, while the built-in sum function works with lists of numbers. The same holds true for the built in `min` and `max` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+--------+--------+\n",
      "|            avg_1|avg_2|            avg_3|min(hwy)|max(hwy)|\n",
      "+-----------------+-----+-----------------+--------+--------+\n",
      "|23.44017094017094|23.44|23.44017094017094|      12|      44|\n",
      "+-----------------+-----+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try out some functions:\n",
    "\n",
    "mpg.select(\n",
    "    (sum(mpg.hwy) / count(mpg.hwy)).alias('avg_1'), \n",
    "    round(avg(mpg.hwy), 2).alias('avg_2'), \n",
    "    mean(mpg.hwy).alias('avg_3'),\n",
    "    min(mpg.hwy), \n",
    "    max(mpg.hwy)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---------------------------+\n",
      "|manufacturer|model|concat(manufacturer, model)|\n",
      "+------------+-----+---------------------------+\n",
      "|        audi|   a4|                     audia4|\n",
      "|        audi|   a4|                     audia4|\n",
      "|        audi|   a4|                     audia4|\n",
      "|        audi|   a4|                     audia4|\n",
      "|        audi|   a4|                     audia4|\n",
      "+------------+-----+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    mpg.manufacturer, \n",
    "    mpg.model,\n",
    "    concat(mpg.manufacturer, mpg.model)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use a string literal as part of our select, we'll need to use the `lit` function, otherwise spark will try to resolve our string as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|cyl|concat(cyl,  cylinders)|\n",
      "+---+-----------------------+\n",
      "|  4|            4 cylinders|\n",
      "|  4|            4 cylinders|\n",
      "|  4|            4 cylinders|\n",
      "|  4|            4 cylinders|\n",
      "|  6|            6 cylinders|\n",
      "+---+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare\n",
    "\n",
    "mpg.select(\n",
    "    mpg.cyl,\n",
    "    concat(mpg.cyl, lit(\" cylinders\"))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the concatenation of the number of cylinders (the value from the cyl column) and the string literal \" cylinders\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More pyspark functions for string manipulation\n",
    "- `regexp_extract`\n",
    "- `regexp_replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In order to demonstrate these functions we'll create a dataframe with some text data.\n",
    "\n",
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_extract`: specify a regular expression with at least one capture group, and create a new column based on the contents of a capture group.\n",
    "- first argument: the name of the string column to extract from.\n",
    "- second argument: the regular expression itself.\n",
    "- last argument: specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\", \n",
    "    regexp_extract(\"address\", \n",
    "                   r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "    regexp_extract(\"address\", \n",
    "                   r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`regexp_replace` lets us make substitutions based on a regular expression.\n",
    "\n",
    "Below, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.select(\n",
    "    \"address\", \n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
