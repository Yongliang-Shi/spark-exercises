{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the pyspark session\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "These exercises should go in a notebook or script named wrangle. Add, commit, and push your changes.\n",
    "\n",
    "This exercises uses the `case.csv`, `dept.csv`, and `source.csv` files from the san antonio 311 call dataset.\n",
    "\n",
    "1. Read the case, department, and source data into their own spark dataframes.\n",
    "2. Let's see **how writing to the local disk works in spark**:\n",
    "    - Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "    - Inspect your folder structure. What do you notice?\n",
    "3. Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read the case, department, and source data into their own spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, SLA_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read case.csv\n",
    "\n",
    "df_case = spark.read.csv(\"data/case.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "df_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dept.csv\n",
    "\n",
    "df_source = (spark.read.format(\"csv\")\n",
    "             .option(\"sep\", \",\")\n",
    "             .option(\"inferSchema\", True)\n",
    "             .option(\"header\", True)\n",
    "             .load(\"data/source.csv\")\n",
    "            )\n",
    "\n",
    "df_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dept_division: string, dept_name: string, standardized_dept_name: string, dept_subject_to_SLA: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dept.csv\n",
    "\n",
    "df_dept = spark.read.csv(\"data/dept.csv\", sep=\",\", header=True, inferSchema=True)\n",
    "df_dept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write to the local disk works in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the df_source spark dataframe to csv\n",
    "\n",
    "# df_source.write.csv(\"source_csv\", mode=\"overwrite\")\n",
    "# df_source.write.json(\"source_json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaways**\n",
    "1. Two folders are created respectively, instead of .csv and .json files.\n",
    "2. Inside each folder, there are two files: _SUCCESS and part-***** files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- SLA_due_date: string (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " SLA_due_date         | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the df_case\n",
    "\n",
    "df_case.printSchema()\n",
    "df_case.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column SLA_due_date to case_due_date\n",
    "df_case = df_case.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Handle dtypes: \n",
    "    # convert the Yes/No values to boolean using expr() method\n",
    "    # cast column council_district to integer\n",
    "\n",
    "df_case = (\n",
    "    df_case.withColumn(\"case_late\", expr(\"case_late == 'YES'\"))\n",
    "    .withColumn(\"case_closed\", expr(\"case_closed == 'YES'\"))\n",
    "    .withColumn(\"council_district\", format_string(\"%03d\", col(\"council_district\")))\n",
    ")\n",
    "\n",
    "df_case = df_case.withColumn(\"council_district\", df_case.council_district.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle dates\n",
    "    # Convert the string datetime to standard format\n",
    "\n",
    "# Define the format\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "\n",
    "# Use to_timestamp method to standardize the datetime\n",
    "\n",
    "df_case = (\n",
    "    df_case.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", to_timestamp(\"case_closed_date\", fmt))\n",
    "    .withColumn(\"case_due_date\", to_timestamp(\"case_due_date\", fmt))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-08 10:38:00\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "# Add features\n",
    "\n",
    "# Compute the max date\n",
    "max_date = df_case.select(max(\"case_closed_date\")).first()[0]\n",
    "print(max_date)\n",
    "print(type(max_date))\n",
    "\n",
    "df_case = (\n",
    "    df_case.withColumn(\"num_weeks_late\", expr(\"num_days_late / 7 AS num_weeks_late\"))\n",
    "    .withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "    .withColumn(\"case_age\", datediff(lit(max_date), \"case_opened_date\"))\n",
    "    .withColumn(\"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\"))\n",
    "    .withColumn(\"case_lifetime\", \n",
    "                when(expr(\"!case_closed\"), col(\"case_age\")).otherwise(col(\"days_to_closed\"))\n",
    "               )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipcode(string):\n",
    "    regexp = r'\\d+$'\n",
    "    return re.search(regexp, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode(\"2315  EL PASO ST, San Antonio, 78207\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join dept.csv\n",
    "\n",
    "df = (\n",
    "    df_case.join(df_dept, \"dept_division\", \"left\")\n",
    "#     .drop(df_dept.dept_name)\n",
    "#     .drop(df_case.dept_division)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_source\n",
    "\n",
    "df = df.join(df_source, \"source_id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855269"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: timestamp (nullable = true)\n",
      " |-- case_closed_date: timestamp (nullable = true)\n",
      " |-- case_due_date: timestamp (nullable = true)\n",
      " |-- case_late: boolean (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: boolean (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      " |-- num_weeks_late: double (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- case_age: integer (nullable = true)\n",
      " |-- days_to_closed: integer (nullable = true)\n",
      " |-- case_lifetime: integer (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- dept_subject_to_SLA: boolean (nullable = true)\n",
      " |-- source_username: string (nullable = true)\n",
      "\n",
      "-RECORD 0------------------------------------\n",
      " source_id            | svcCRMLS             \n",
      " dept_division        | Field Operations     \n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 12:29:00  \n",
      " case_due_date        | 2020-09-26 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 219                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " dept_name            | Animal Care Services \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      " source_username      | svcCRMLS             \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to querry our dataframe with Spark SQL\n",
    "\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How old is the latest (in terms of days past SLA) currently open issue? How long has the oldest (in terms of days since opened) currently opened issue been open?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 8, 8, 10, 38)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = (\n",
    "    df.filter(col(\"case_closed\") == False) # Currently open issue\n",
    "    .sort(col(\"case_opened_date\").desc())  # the latest issue\n",
    "    .first()\n",
    ")\n",
    "\n",
    "row.case_opened_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|days_past_due|\n",
      "+-------------+\n",
      "|         1416|\n",
      "|         1416|\n",
      "|         1416|\n",
      "|         1415|\n",
      "|         1413|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark SQL to answer this question\n",
    "\n",
    "spark.sql('''\n",
    "SELECT DATEDIFF(current_timestamp, case_due_date) AS days_past_due\n",
    "FROM df\n",
    "WHERE NOT case_closed\n",
    "ORDER BY days_past_due DESC\n",
    "LIMIT 5\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How many Stray Animal cases are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27361"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the number of rows with Stray Animal cases\n",
    "\n",
    "df.filter(df_case.service_request_type == \"Stray Animal\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   27361|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spark SQL\n",
    "\n",
    "spark.sql('''\n",
    "SELECT count(*)\n",
    "FROM df\n",
    "WHERE service_request_type == \"Stray Animal\"\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How many service requests that are assigned to the Field Operations department (dept_division) are not classified as \"Officer Standby\" request type (service_request_type)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116295"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df.filter(expr(\"dept_division == 'Field Operations'\"))\n",
    "    .where(expr('''service_request_type != \"Officer Standby\"'''))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using spark sql\n",
    "\n",
    "spark.sql('''\n",
    "SELECT count(*)\n",
    "FROM df\n",
    "WHERE service_request_type != \"Officer Standby\"\n",
    "AND dept_division == \"Field Operations\"\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convert the council_district column to a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, dept_division: string, case_id: int, case_opened_date: timestamp, case_closed_date: timestamp, case_due_date: timestamp, case_late: boolean, num_days_late: double, case_closed: boolean, service_request_type: string, SLA_days: double, case_status: string, request_address: string, council_district: int, num_weeks_late: double, zipcode: string, case_age: int, days_to_closed: int, case_lifetime: int, dept_name: string, department: string, dept_subject_to_SLA: boolean, source_username: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use cast() method\n",
    "\n",
    "df.withColumn(\"council_district\", col(\"council_district\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extract the year from the case_closed_date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|   case_closed_date|year(case_closed_date)|\n",
      "+-------------------+----------------------+\n",
      "|2018-01-01 12:29:00|                  2018|\n",
      "|2018-01-03 08:11:00|                  2018|\n",
      "|2018-01-02 07:57:00|                  2018|\n",
      "|2018-01-02 08:13:00|                  2018|\n",
      "|2018-01-01 13:29:00|                  2018|\n",
      "+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    \"case_closed_date\",\n",
    "    year(\"case_closed_date\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Convert num_days_late from days to hours in new columns num_hours_late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     num_hours_late|      num_days_late|\n",
      "+-------------------+-------------------+\n",
      "|     -23964.2102784| -998.5087616000001|\n",
      "|-48.302500007999996|-2.0126041669999997|\n",
      "|      -72.536111112|       -3.022337963|\n",
      "|      -360.27555552|       -15.01148148|\n",
      "|  8.931944448000001|0.37216435200000003|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"num_hours_late\",\n",
    "    df.num_days_late * 24\n",
    ").select(\n",
    "    \"num_hours_late\",\n",
    "    \"num_days_late\"    \n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Join the case data with the source and department data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Are there any cases that do not have a request source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spark sql\n",
    "\n",
    "spark.sql('''\n",
    "select count(*)\n",
    "from df\n",
    "where source_id == null\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition, true, [id=#1015]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) LocalTableScan <empty>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain the process\n",
    "\n",
    "spark.sql('''\n",
    "select count(*)\n",
    "from df\n",
    "where source_id == null\n",
    "''').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|sum(is_null)|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark dataframe\n",
    "\n",
    "(\n",
    "    df.select(df.source_id.isNull().cast('int').alias(\"is_null\"))\n",
    "    .agg(sum('is_null'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[], functions=[sum(cast(is_null#1042 as bigint))])\n",
      "+- Exchange SinglePartition, true, [id=#1302]\n",
      "   +- *(3) HashAggregate(keys=[], functions=[partial_sum(cast(is_null#1042 as bigint))])\n",
      "      +- *(3) Project [cast(isnull(source_id#27) as int) AS is_null#1042]\n",
      "         +- *(3) BroadcastHashJoin [source_id#27], [source_id#60], LeftOuter, BuildRight\n",
      "            :- *(3) Project [source_id#27]\n",
      "            :  +- *(3) BroadcastHashJoin [dept_division#23], [dept_division#80], LeftOuter, BuildRight\n",
      "            :     :- FileScan csv [dept_division#23,source_id#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/case.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dept_division:string,source_id:string>\n",
      "            :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#1288]\n",
      "            :        +- *(1) Project [dept_division#80]\n",
      "            :           +- *(1) Filter isnotnull(dept_division#80)\n",
      "            :              +- FileScan csv [dept_division#80] Batched: false, DataFilters: [isnotnull(dept_division#80)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/dept.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dept_division)], ReadSchema: struct<dept_division:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#1296]\n",
      "               +- *(2) Project [source_id#60]\n",
      "                  +- *(2) Filter isnotnull(source_id#60)\n",
      "                     +- FileScan csv [source_id#60] Batched: false, DataFilters: [isnotnull(source_id#60)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/source.csv], PartitionFilters: [], PushedFilters: [IsNotNull(source_id)], ReadSchema: struct<source_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain\n",
    "\n",
    "(\n",
    "    df.select(df.source_id.isNull().cast('int').alias(\"is_null\"))\n",
    "    .agg(sum('is_null'))\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What are the top 10 service request types in terms of number of requests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------+\n",
      "|service_request_type            |num_request|\n",
      "+--------------------------------+-----------+\n",
      "|No Pickup                       |89210      |\n",
      "|Overgrown Yard/Trash            |66403      |\n",
      "|Bandit Signs                    |32968      |\n",
      "|Damaged Cart                    |31163      |\n",
      "|Front Or Side Yard Parking      |28920      |\n",
      "|Stray Animal                    |27361      |\n",
      "|Aggressive Animal(Non-Critical) |25492      |\n",
      "|Cart Exchange Request           |22608      |\n",
      "|Junk Vehicle On Private Property|21649      |\n",
      "|Pot Hole Repair                 |20827      |\n",
      "+--------------------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark sql\n",
    "\n",
    "spark.sql('''\n",
    "select service_request_type, count(*) as num_request\n",
    "from df\n",
    "group by service_request_type\n",
    "order by num_request desc\n",
    "''').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Sort [num_request#1289L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(num_request#1289L DESC NULLS LAST, 200), true, [id=#2205]\n",
      "   +- *(4) HashAggregate(keys=[service_request_type#24], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(service_request_type#24, 200), true, [id=#2201]\n",
      "         +- *(3) HashAggregate(keys=[service_request_type#24], functions=[partial_count(1)])\n",
      "            +- *(3) Project [service_request_type#24]\n",
      "               +- *(3) BroadcastHashJoin [source_id#27], [source_id#60], LeftOuter, BuildRight\n",
      "                  :- *(3) Project [service_request_type#24, source_id#27]\n",
      "                  :  +- *(3) BroadcastHashJoin [dept_division#23], [dept_division#80], LeftOuter, BuildRight\n",
      "                  :     :- FileScan csv [dept_division#23,service_request_type#24,source_id#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/case.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dept_division:string,service_request_type:string,source_id:string>\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#2187]\n",
      "                  :        +- *(1) Project [dept_division#80]\n",
      "                  :           +- *(1) Filter isnotnull(dept_division#80)\n",
      "                  :              +- FileScan csv [dept_division#80] Batched: false, DataFilters: [isnotnull(dept_division#80)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/dept.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dept_division)], ReadSchema: struct<dept_division:string>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#2195]\n",
      "                     +- *(2) Project [source_id#60]\n",
      "                        +- *(2) Filter isnotnull(source_id#60)\n",
      "                           +- FileScan csv [source_id#60] Batched: false, DataFilters: [isnotnull(source_id#60)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/source.csv], PartitionFilters: [], PushedFilters: [IsNotNull(source_id)], ReadSchema: struct<source_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select service_request_type, count(*) as num_request\n",
    "from df\n",
    "group by service_request_type\n",
    "order by num_request desc\n",
    "''').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----+\n",
      "|service_request_type            |count|\n",
      "+--------------------------------+-----+\n",
      "|No Pickup                       |89210|\n",
      "|Overgrown Yard/Trash            |66403|\n",
      "|Bandit Signs                    |32968|\n",
      "|Damaged Cart                    |31163|\n",
      "|Front Or Side Yard Parking      |28920|\n",
      "|Stray Animal                    |27361|\n",
      "|Aggressive Animal(Non-Critical) |25492|\n",
      "|Cart Exchange Request           |22608|\n",
      "|Junk Vehicle On Private Property|21649|\n",
      "|Pot Hole Repair                 |20827|\n",
      "+--------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark dataframe\n",
    "\n",
    "(\n",
    "    df.groupby(\"service_request_type\")\n",
    "    .count()\n",
    "    .sort(col(\"count\").desc())\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Sort [count#1320L DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(count#1320L DESC NULLS LAST, 200), true, [id=#2292]\n",
      "   +- *(4) HashAggregate(keys=[service_request_type#24], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(service_request_type#24, 200), true, [id=#2288]\n",
      "         +- *(3) HashAggregate(keys=[service_request_type#24], functions=[partial_count(1)])\n",
      "            +- *(3) Project [service_request_type#24]\n",
      "               +- *(3) BroadcastHashJoin [source_id#27], [source_id#60], LeftOuter, BuildRight\n",
      "                  :- *(3) Project [service_request_type#24, source_id#27]\n",
      "                  :  +- *(3) BroadcastHashJoin [dept_division#23], [dept_division#80], LeftOuter, BuildRight\n",
      "                  :     :- FileScan csv [dept_division#23,service_request_type#24,source_id#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/case.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dept_division:string,service_request_type:string,source_id:string>\n",
      "                  :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#2274]\n",
      "                  :        +- *(1) Project [dept_division#80]\n",
      "                  :           +- *(1) Filter isnotnull(dept_division#80)\n",
      "                  :              +- FileScan csv [dept_division#80] Batched: false, DataFilters: [isnotnull(dept_division#80)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/dept.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dept_division)], ReadSchema: struct<dept_division:string>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#2282]\n",
      "                     +- *(2) Project [source_id#60]\n",
      "                        +- *(2) Filter isnotnull(source_id#60)\n",
      "                           +- FileScan csv [source_id#60] Batched: false, DataFilters: [isnotnull(source_id#60)], Format: CSV, Location: InMemoryFileIndex[file:/Users/mms3-pro/codeup-data-science/spark-exercises/data/source.csv], PartitionFilters: [], PushedFilters: [IsNotNull(source_id)], ReadSchema: struct<source_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(\n",
    "    df.groupby(\"service_request_type\")\n",
    "    .count()\n",
    "    .sort(col(\"count\").desc())\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. What are the top 10 service request types in terms of average days late?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------+\n",
      "|service_request_type                  |avg_days_late     |\n",
      "+--------------------------------------+------------------+\n",
      "|Zoning: Recycle Yard                  |210.89201994318182|\n",
      "|Zoning: Junk Yards                    |200.20517608494276|\n",
      "|Structure/Housing Maintenance         |190.20707698509807|\n",
      "|Donation Container Enforcement        |171.09115313942615|\n",
      "|Storage of Used Mattress              |163.96812829714287|\n",
      "|Labeling for Used Mattress            |162.43032902285717|\n",
      "|Record Keeping of Used Mattresses     |153.99724039428568|\n",
      "|Signage Requied for Sale of Used Mattr|151.63868055333333|\n",
      "|Traffic Signal Graffiti               |137.64583330000002|\n",
      "|License Requied Used Mattress Sales   |128.79828704142858|\n",
      "+--------------------------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select service_request_type, avg(num_days_late) as avg_days_late\n",
    "from df\n",
    "where case_late == True\n",
    "group by service_request_type\n",
    "order by avg_days_late desc\n",
    "''').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Does number of days late depend on department?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------------+\n",
      "|dept_name                |avg_days_late     |\n",
      "+-------------------------+------------------+\n",
      "|Metro Health             |6.494699602827871 |\n",
      "|Solid Waste Management   |7.1471727895574135|\n",
      "|Trans & Cap Improvements |10.662950455078674|\n",
      "|Parks and Recreation     |22.427807192724135|\n",
      "|Animal Care Services     |23.4467296347382  |\n",
      "|Code Enforcement Services|48.085502375106884|\n",
      "|Development Services     |67.22248485687263 |\n",
      "|Customer Service         |88.18248182589822 |\n",
      "|null                     |210.89201994318182|\n",
      "+-------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark sql\n",
    "\n",
    "spark.sql('''\n",
    "select dept_name, avg(num_days_late) as avg_days_late\n",
    "from df\n",
    "where case_late == True\n",
    "group by dept_name\n",
    "order by avg_days_late\n",
    "''').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------+\n",
      "|dept_name                |avg_days_late|\n",
      "+-------------------------+-------------+\n",
      "|Metro Health             |6.5          |\n",
      "|Solid Waste Management   |7.1          |\n",
      "|Trans & Cap Improvements |10.7         |\n",
      "|Parks and Recreation     |22.4         |\n",
      "|Animal Care Services     |23.4         |\n",
      "|Code Enforcement Services|48.1         |\n",
      "|Development Services     |67.2         |\n",
      "|Customer Service         |88.2         |\n",
      "|null                     |210.9        |\n",
      "+-------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use spark dataframe\n",
    "\n",
    "(\n",
    "    df.filter(\"case_late\")\n",
    "    .groupby(\"dept_name\")\n",
    "    .agg(mean(\"num_days_late\").alias(\"avg_days_late\"))\n",
    "    .sort(\"avg_days_late\")\n",
    "    .withColumn(\"avg_days_late\", round(col(\"avg_days_late\"), 1))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. How do number of days late depend on department and request type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------------+-------------------+\n",
      "|dept_name               |service_request_type                  |avg_days_late      |\n",
      "+------------------------+--------------------------------------+-------------------+\n",
      "|City Council            |Request for Research/Information      |null               |\n",
      "|Trans & Cap Improvements|Engineering Design                    |-1413.83706        |\n",
      "|Trans & Cap Improvements|Signal Timing Modification By Engineer|-1351.9919107954547|\n",
      "|Animal Care Services    |Stray Animal                          |-998.8086340218391 |\n",
      "|Parks and Recreation    |Major Park Improvement Install        |-278.26410401191515|\n",
      "+------------------------+--------------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spark sql\n",
    "\n",
    "spark.sql('''\n",
    "select dept_name, service_request_type, avg(num_days_late) as avg_days_late\n",
    "from df\n",
    "where case_closed == True\n",
    "group by dept_name, service_request_type\n",
    "order by avg_days_late\n",
    "''').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------------------+-------------+-------+\n",
      "|dept_name               |service_request_type                  |avg_days_late|n_cases|\n",
      "+------------------------+--------------------------------------+-------------+-------+\n",
      "|City Council            |Request for Research/Information      |null         |5      |\n",
      "|Trans & Cap Improvements|Engineering Design                    |-1413.8      |1      |\n",
      "|Trans & Cap Improvements|Signal Timing Modification By Engineer|-1352.0      |22     |\n",
      "|Animal Care Services    |Stray Animal                          |-998.8       |27346  |\n",
      "|Parks and Recreation    |Major Park Improvement Install        |-278.3       |271    |\n",
      "+------------------------+--------------------------------------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.filter(\"case_closed\")\n",
    "#     .filter(\"case_late\")\n",
    "    .groupby(\"dept_name\", \"service_request_type\")\n",
    "    .agg(avg(\"num_days_late\").alias(\"avg_days_late\"), count(\"*\").alias(\"n_cases\"))\n",
    "    .withColumn(\"avg_days_late\", round(col(\"avg_days_late\"), 1))\n",
    "    .sort(asc(\"avg_days_late\"))\n",
    "    .show(5, truncate=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
